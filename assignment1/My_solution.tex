\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{CJKutf8}  %输入中文
\usepackage{amsmath}  %数学公式
\usepackage{amssymb} 
\usepackage{setspace} %使用间距宏包
\usepackage{geometry} %设置页面边距、页面大小的宏包

%\geometry{a4paper,scale=0.8} %版心占页面比例为80%
\geometry{a4paper,left=2.5cm,right=2.5cm,top=1cm,bottom=2cm}

\title{
	\begin{LARGE}
		\textbf{Solutions to assignment1 of CS224n}
	\end{LARGE}
	}	

\author{陈承勃}

\date{May 22, 2018}

\begin{document}
\begin{CJK}{UTF8}{gbsn}
\maketitle
\end{CJK}
\begin{spacing}{1.5} %设置行间距x[ix] -

\section{Softmax}
(a) Omitted. 
(b) See \textbf{q1\_softmax.py}

\section{Neural Network Basics}
\paragraph{(a)} $ \sigma'(x)=\sigma(x)\sigma(1-x) $
\paragraph{(b)} 
Assume k is the correct class, then \newline
$ CE(\boldsymbol{y},\boldsymbol{\widehat{y}})=
-y_{k}\log \widehat{y_{k}}=-\log \widehat{y_{k}}
=-\log \frac{\exp(\boldsymbol{\theta}_k)}{\begin{matrix} \sum_{i} \exp(\boldsymbol{\theta}_i) \end{matrix}}
=-\boldsymbol{\theta}_{k}+\log \begin{matrix} \sum_{i} \exp(\boldsymbol{\theta}_{i}) \end{matrix} $.
\newline
$ \boldsymbol{\therefore} $
$ \frac{\partial CE(\boldsymbol{y},\boldsymbol{\widehat{y}})}{\partial \boldsymbol{\theta}_{k}}
=-1+\frac{\exp(\boldsymbol{\theta}_k)}{\begin{matrix} \sum_{i} \exp(\boldsymbol{\theta}_{i}) \end{matrix}}
=\widehat{y_{k}}-1 $, 
\newline
$ \frac{\partial CE(\boldsymbol{y},\boldsymbol{\widehat{y}})}{\partial \boldsymbol{\theta}_{j}}
=\frac{\exp(\boldsymbol{\theta}_j)}{\begin{matrix} \sum_{i} \exp(\boldsymbol{\theta}_{i}) \end{matrix}}
=\widehat{y_{j}},\ j\neq k $.
\newline
$ \boldsymbol{\therefore} $
$ \frac{\partial CE(\boldsymbol{y},\boldsymbol{\widehat{y}})}{\partial \boldsymbol{\theta}}=\boldsymbol{\widehat{y}}-\boldsymbol{y} $
\paragraph{(c)}
The forward propagation steps: \newline
$ \boldsymbol{Z_{1}}=\boldsymbol{xW_{1}}+\boldsymbol{b_{1}}, \quad
\boldsymbol{h}=sigmoid(\boldsymbol{Z_{1}})$ \newline
$ \boldsymbol{Z_{2}}=\boldsymbol{hW_{2}}+\boldsymbol{b_{2}}, \quad
\boldsymbol{\widehat{y}}=sigmoid(\boldsymbol{Z_{2}})$ \newline
$ \boldsymbol{J}=CE(\boldsymbol{y}, \boldsymbol{\widehat{y}}) $ \newline
The backward propagation: \newline
$ \frac{\partial \boldsymbol{J}}{\partial \boldsymbol{Z_2}}
=\boldsymbol{\widehat{y}}-\boldsymbol{y} \triangleq \boldsymbol{\delta_1}, \quad  
\frac{\partial \boldsymbol{J}}{\partial \boldsymbol{h}}
=\boldsymbol{\delta_1} \boldsymbol{W_2 ^ \mathrm{T}} \triangleq \boldsymbol{\delta_2}$ \newline
$ \frac{\partial \boldsymbol{J}}{\partial \boldsymbol{Z_1}}
=\boldsymbol{\delta_2} \ast \sigma'(\boldsymbol{Z_1})\triangleq \boldsymbol{\delta_3}, \ \ast$ denotes element-wise product. \newline
$ \frac{\partial \boldsymbol{J}}{\partial \boldsymbol{x}}
=\boldsymbol{\delta_3}\boldsymbol{W_1 ^ \mathrm{T}} $
\paragraph{(d)}
$ (1+D_x) \times H + (1+H) \times D_y $
\paragraph{(e)} See \textbf{q2\_sigmoid.py}
\paragraph{(f)} See \textbf{q2\_gradcheck.py}
\paragraph{(g)} See \textbf{q2\_neural.py}

\section{word2vec}
\paragraph{(a)}

\end{spacing}
\end{document}
